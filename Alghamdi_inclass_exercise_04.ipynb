{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alghamdi_inclass_exercise_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AsmaaAlghamd1/Asmaa_INFO5731_-Spring2021/blob/main/Alghamdi_inclass_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total, 2/9/2021)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/info5731_spring2021/blob/main/class_exercises/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR0L3_CreM_A",
        "outputId": "4f3791fb-23cd-4449-c99b-9ee44a922db3"
      },
      "source": [
        "My_file = open(\"01-05-1  Adams v Tanner.txt\", \"r\")\n",
        "Contents =My_file.read()\n",
        "print(\"Number of sentences :\", Contents.count('.'))\n",
        "print(\"Number of words :\",len(Contents.split(\" \")))\n",
        "print(\"Number of characters :\",len(Contents))\n",
        "\n",
        "\n"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sentences : 291\n",
            "Number of words : 3575\n",
            "Number of characters : 20454\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqPa9Wlwd2kj",
        "outputId": "dfed3557-a2f6-4ecb-f8d9-4119f455bb75"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "import string\n",
        "enStops = sw.words('english')\n",
        "\n",
        "print(\"_____________________\")\n",
        "stopwords = len([x for x in Contents if x in enStops])\n",
        "print(\"Number of stopwords:\",stopwords)\n",
        "print(\"_____________________\")\n",
        "\n",
        "\n",
        "Punctuations=len([x for x in Contents if x in string.punctuation])\n",
        "print(\"Number of Punctuations:\",Punctuations)\n",
        "print(\"_____________________\")\n",
        "\n",
        "Numerous= len([x for x in Contents if x.isdigit()])\n",
        "print(\"Number of numerics:\",Numerous)\n",
        "print(\"_____________________\")\n",
        "\n",
        "Upwords=len([x for x in Contents if x.isupper()])\n",
        "\n",
        "print(\"Number of uppercase words:\",Upwords)\n",
        "print(\"_____________________\")\n",
        "\n",
        "lowords=len([x for x in Contents if x.islower()])\n",
        "print(\"Number of lowercase words:\",lowords)\n"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "_____________________\n",
            "Number of stopwords: 7032\n",
            "_____________________\n",
            "Number of Punctuations: 774\n",
            "_____________________\n",
            "Number of numerics: 356\n",
            "_____________________\n",
            "Number of uppercase words: 695\n",
            "_____________________\n",
            "Number of lowercase words: 14852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpw3MEoJd7RO",
        "outputId": "0a3a59ca-f435-4275-a2d6-fb18bb4f1a51"
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "from textblob import Word\n",
        "from textblob import TextBlob\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer as PS\n",
        "\n",
        "My_data=[]\n",
        "for Y in Contents.split(\".\"):\n",
        "  data1=Y.replace('\\r\\n',' ')\n",
        "  My_data.append(data1)\n",
        "df=pd.DataFrame(My_data,columns=['Original Text'])"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        },
        "id": "P24_ygj8sD_g",
        "outputId": "afe0a921-a858-4103-fe00-ca4f4c37714b"
      },
      "source": [
        "stop = sw.words('english')\n",
        "PS = PorterStemmer()\n",
        "A=df['lower Case']=df['Original Text'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
        "\n",
        "B=df['Punctuation Removal']=df['lower Case'].apply(lambda  x: \" \".join(x for x in x.split() if x not in string.punctuation))\n",
        "\n",
        "C=df['Stopwords Removal']=df['Punctuation Removal'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "freq= pd.Series(' '.join(df['Stopwords Removal']).split()).value_counts()[:10]\n",
        "\n",
        "\n",
        "freq=list(freq.index)\n",
        "E=df['Common words removal']=df['Stopwords Removal'].apply(lambda x:\" \".join(x for x in x.split() if x not in freq))\n",
        "\n",
        "rare=pd.Series(\" \".join(df['Stopwords Removal']).split()).value_counts()[-10:]\n",
        "\n",
        "rare=list(rare.index)\n",
        "F=df['Rare Words removal']=df['Common words removal'].apply(lambda x:\" \".join(x for x in x.split() if x not in rare))\n",
        "\n",
        "G=df['Correction of Spelling']=df['Rare Words removal'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "\n",
        "H=df['Tokenization']=df['Correction of Spelling'].apply(lambda x: TextBlob(x).words)\n",
        "\n",
        "J=df['Stemming']=df['Tokenization'].apply(lambda x: \" \".join([PS.stem(word) for word in x]))\n",
        "\n",
        "K=df['lemmatization']=df[\"Stemming\"].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "df"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Original Text</th>\n",
              "      <th>lower Case</th>\n",
              "      <th>Punctuation Removal</th>\n",
              "      <th>Stopwords Removal</th>\n",
              "      <th>Common words removal</th>\n",
              "      <th>Rare Words removal</th>\n",
              "      <th>Correction of Spelling</th>\n",
              "      <th>Tokenization</th>\n",
              "      <th>Stemming</th>\n",
              "      <th>lemmatization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n5 Ala</td>\n",
              "      <td>5 ala</td>\n",
              "      <td>5 ala</td>\n",
              "      <td>5 ala</td>\n",
              "      <td>5 ala</td>\n",
              "      <td>5 ala</td>\n",
              "      <td>5 all</td>\n",
              "      <td>[5, all]</td>\n",
              "      <td>5 all</td>\n",
              "      <td>5 all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>740\\nSupreme Court of Alabama</td>\n",
              "      <td>740 supreme court of alabama</td>\n",
              "      <td>740 supreme court of alabama</td>\n",
              "      <td>740 supreme court alabama</td>\n",
              "      <td>740 supreme alabama</td>\n",
              "      <td>740 supreme alabama</td>\n",
              "      <td>740 supreme alabama</td>\n",
              "      <td>[740, supreme, alabama]</td>\n",
              "      <td>740 suprem alabama</td>\n",
              "      <td>740 suprem alabama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\nADAMS\\nv</td>\n",
              "      <td>adams v</td>\n",
              "      <td>adams v</td>\n",
              "      <td>adams v</td>\n",
              "      <td>adams</td>\n",
              "      <td>adams</td>\n",
              "      <td>adams</td>\n",
              "      <td>[adams]</td>\n",
              "      <td>adam</td>\n",
              "      <td>adam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\nTANNER AND HORTON</td>\n",
              "      <td>tanner and horton</td>\n",
              "      <td>tanner and horton</td>\n",
              "      <td>tanner horton</td>\n",
              "      <td>tanner horton</td>\n",
              "      <td>tanner horton</td>\n",
              "      <td>manner norton</td>\n",
              "      <td>[manner, norton]</td>\n",
              "      <td>manner norton</td>\n",
              "      <td>manner norton</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\nJune Term, 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "      <td>[june, term, 1843]</td>\n",
              "      <td>june term 1843</td>\n",
              "      <td>june term 1843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>\\nCase\\n2\\nFilings\\nThere are no Filings for t...</td>\n",
              "      <td>case 2 filings there are no filings for this c...</td>\n",
              "      <td>case 2 filings there are no filings for this c...</td>\n",
              "      <td>case 2 filings filings citation</td>\n",
              "      <td>case 2 filings filings citation</td>\n",
              "      <td>case 2 filings filings citation</td>\n",
              "      <td>case 2 filing filing situation</td>\n",
              "      <td>[case, 2, filing, filing, situation]</td>\n",
              "      <td>case 2 file file situat</td>\n",
              "      <td>case 2 file file situat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>\\nNegative Treatment\\nThere are no Negative Tr...</td>\n",
              "      <td>negative treatment there are no negative treat...</td>\n",
              "      <td>negative treatment there are no negative treat...</td>\n",
              "      <td>negative treatment negative treatment results ...</td>\n",
              "      <td>negative treatment negative treatment results ...</td>\n",
              "      <td>negative treatment negative treatment results ...</td>\n",
              "      <td>negative treatment negative treatment results ...</td>\n",
              "      <td>[negative, treatment, negative, treatment, res...</td>\n",
              "      <td>neg treatment neg treatment result situat</td>\n",
              "      <td>neg treatment neg treatment result situat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>\\nHistory\\nThere are no History results for th...</td>\n",
              "      <td>history there are no history results for this ...</td>\n",
              "      <td>history there are no history results for this ...</td>\n",
              "      <td>history history results citation</td>\n",
              "      <td>history history results citation</td>\n",
              "      <td>history history results citation</td>\n",
              "      <td>history history results situation</td>\n",
              "      <td>[history, history, results, situation]</td>\n",
              "      <td>histori histori result situat</td>\n",
              "      <td>histori histori result situat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>\\n</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>[]</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>292 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Original Text  ...                              lemmatization\n",
              "0                                              \\n5 Ala  ...                                      5 all\n",
              "1                        740\\nSupreme Court of Alabama  ...                         740 suprem alabama\n",
              "2                                           \\nADAMS\\nv  ...                                       adam\n",
              "3                                  \\nTANNER AND HORTON  ...                              manner norton\n",
              "4                                    \\nJune Term, 1843  ...                             june term 1843\n",
              "..                                                 ...  ...                                        ...\n",
              "287                                                     ...                                           \n",
              "288  \\nCase\\n2\\nFilings\\nThere are no Filings for t...  ...                    case 2 file file situat\n",
              "289  \\nNegative Treatment\\nThere are no Negative Tr...  ...  neg treatment neg treatment result situat\n",
              "290  \\nHistory\\nThere are no History results for th...  ...              histori histori result situat\n",
              "291                                                 \\n  ...                                           \n",
              "\n",
              "[292 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhmTyADCh18S"
      },
      "source": [
        "df.to_csv('Cleaned Data.csv',index=False)\n",
        "\n"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgNiiHIv7EfO"
      },
      "source": [
        "new_text_list=[]\n",
        "with open('Cleaned Data.csv') as M:\n",
        "    writer = csv.writer(M)\n",
        "  \n",
        "    writer.writerows([[item] for item in new_text_list])\n"
      ],
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3qottvkxgcy",
        "outputId": "554d3eb0-a302-45b3-f20a-6fdc5cb026c9"
      },
      "source": [
        "Cleansent=K.apply(lambda x:(str(x).split(\" \" )))\n",
        "Cleansent.columns=['Clened sentences']\n",
        "Cleansent"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                              [5, all]\n",
              "1                                [740, suprem, alabama]\n",
              "2                                                [adam]\n",
              "3                                      [manner, norton]\n",
              "4                                    [june, term, 1843]\n",
              "                             ...                       \n",
              "287                                                  []\n",
              "288                       [case, 2, file, file, situat]\n",
              "289    [neg, treatment, neg, treatment, result, situat]\n",
              "290                  [histori, histori, result, situat]\n",
              "291                                                  []\n",
              "Name: Stemming, Length: 292, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P73blRdKxGmz",
        "outputId": "58f595fb-ec30-4a8b-b8f6-056c0d4ce9d2"
      },
      "source": [
        "##Calculate the term frequency of all the terms.\n",
        "My_text=K.apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n",
        "My_text.columns=['Terms', 'Terms_frequency ']\n",
        "print(My_text)"
      ],
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Terms  Terms_frequency \n",
            "0          5               8.0\n",
            "1        all              16.0\n",
            "2     suprem               1.0\n",
            "3    alabama               1.0\n",
            "4        740               2.0\n",
            "..       ...               ...\n",
            "731     half               1.0\n",
            "732      rye               1.0\n",
            "733     file               2.0\n",
            "734      neg               2.0\n",
            "735  histori               2.0\n",
            "\n",
            "[736 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k23fdZ4gs036",
        "outputId": "786f38ce-f104-4f1b-ac2d-3ec65879d9a6"
      },
      "source": [
        "## Print out top 10 1-gram\n",
        "\n",
        "from nltk import ngrams\n",
        "import itertools\n",
        "from nltk import bigrams as b\n",
        "\n",
        "Gram= []\n",
        "for data in K:\n",
        "  Gram.append(word_tokenize(data))\n",
        "Nogram = [x for x in Gram if x != []]\n",
        "G = list(itertools.chain.from_iterable(Nogram ))\n",
        "Z = ngrams(G, 1)\n",
        "print(collections.Counter(Z).most_common(10))\n"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(('case',), 24), (('gather',), 22), (('law',), 18), (('grow',), 17), (('all',), 16), (('contract',), 16), (('plaintiff',), 15), (('attach',), 14), (('cotton',), 14), (('2',), 13)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSv6fVhOfFmv",
        "outputId": "c50c9005-d2f4-4945-e612-9b92ac79da66"
      },
      "source": [
        "\n",
        "\n",
        "import re\n",
        "ip=\"260.08.094.109\"\n",
        "\n",
        "Remove_Zero=re.sub('\\.[0]*' ,'.', ip)\n",
        "\n",
        "print(\" IP Address after removing leading zeros is: \\n\", Remove_Zero)\n",
        "\n",
        "\n"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The IP Address after removing leading zeros is:\n",
            " 260.8.94.109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xdJpDx9gjbX",
        "outputId": "8c7c26a3-37e4-48d8-a46d-8b2836190efd"
      },
      "source": [
        "sentence = (\"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\")\n",
        "listy=[]   \n",
        "Count=0  \n",
        "s=(\"\")     \n",
        "for i in sentence:\n",
        "    if i=='2':\n",
        "       Count=1\n",
        "    if not(i.isdigit()) and Count==1:\n",
        "      Count=0                          \n",
        "    if i.isdigit() and Count==1:\n",
        "       s+=i                           \n",
        "    if s!=\"\" and i==\" \":\n",
        "      listy.append(int(s))\n",
        "      s=(\"\")                       \n",
        "    \n",
        "print(listy)                              "
      ],
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2010, 2010, 2019]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85Vko48zrkRe",
        "outputId": "02eedb56-5ab7-4595-ff3e-a14fbace4b3e"
      },
      "source": [
        "import re\n",
        "sentence = (\"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\")\n",
        "\n",
        "Y=re.findall(r'2\\d{3}', sentence) \n",
        "print(Y)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['2010', '2010', '2019']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}